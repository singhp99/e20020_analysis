{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Experimental Preprocessing Pipeline</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importing all Packages</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin #for GridSearchCv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertingFiles():\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    None\n",
    "    \n",
    "    Return\n",
    "    (event_data, event_lengths) (packed np.arrays): data converted into .npy format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, run):\n",
    "        self.run = run\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        file_h5py = h5py.File(X)\n",
    "        keys_ls = list(file_h5py.keys())[0]\n",
    "        keys = file_h5py[keys_ls]\n",
    "        \n",
    "        length_run = len(keys)\n",
    "        event_lengths = np.zeros(length_run,int)\n",
    "        \n",
    "        for i,e in enumerate(keys):\n",
    "            length = len(keys[e])\n",
    "            if length > 0: #no point with zero events \n",
    "                event_lengths[i] = length\n",
    "                \n",
    "        valid_mask = event_lengths > 0\n",
    "        event_lengths = event_lengths[valid_mask] #removing the point with the zero point cloud\n",
    "        num_valid = np.sum(valid_mask)\n",
    "        event_data = np.full((num_valid, np.max(event_lengths), 4), np.nan)\n",
    "\n",
    "        valid_idx = 0 #only need to iterate through non-zero events\n",
    "        valid_keys = []\n",
    "        for idx, key in enumerate(tqdm.tqdm(keys, desc=\"Stripping Point Clouds of ID information\")):\n",
    "            dataset = keys[key]\n",
    "            if len(dataset) == 0:\n",
    "                continue\n",
    "            valid_keys.append(key)\n",
    "            for n in range(len(dataset)):\n",
    "                event_data[valid_idx, n] = dataset[n][:4]\n",
    "            valid_idx += 1\n",
    "        \n",
    "        valid_keys = np.array(valid_keys)\n",
    "        np.save(f\"/Volumes/researchEXT/O16/ml models/data_exp_pred/run{self.run}_valid_nonzero_keys.npy\",valid_keys)\n",
    "        \n",
    "        \n",
    "        assert all(len(event_data[i]) == np.max(event_lengths) for i in range(len(event_data))), \"Array length does not match number of events\"\n",
    "        \n",
    "        return (event_data,event_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Outlier Detection</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierDetection:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    None\n",
    "\n",
    "    Return\n",
    "    (event_data,new_event_lengths) (packed np.arrays): outliers removed packed data\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        data,event_lengths = X\n",
    "        event_data = np.full(data.shape, np.nan)\n",
    "        new_event_lengths = np.full_like(event_lengths, np.nan)\n",
    "        tot_count = 0\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            event_points = data[i,:]\n",
    "            condition = ((-270 <= event_points[:, 0]) & (event_points[:, 0] <= 270) &   \n",
    "                (-270 <= event_points[:, 1]) & (event_points[:, 1] <= 270) &\n",
    "                (0 <= event_points[:, 2]) & (event_points[:, 2]  <= 1500))\n",
    "            allowed_points = event_points[condition] #only allows points that are not outliers\n",
    "\n",
    "            event_data[i,:len(allowed_points)] = allowed_points #only assigns the valid points to the new array\n",
    "\n",
    "            new_event_lengths[i] = len(allowed_points)  #original event number minus the number of outliers\n",
    "            tot_count+=event_lengths[i] - new_event_lengths[i]\n",
    "\n",
    "        print(f\"Number of outlier points removed: {tot_count}\") \n",
    "        return (event_data,new_event_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Up/Down Scaling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpDownScaling(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    target_size (int): which is the number of point I want to up/doen sample to \n",
    "\n",
    "    Return\n",
    "    new_data (np.array): up/down sampled data with shape (run_events, target_size,4) \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,target_size,isotope,dimension=4):\n",
    "        self.target_size = target_size\n",
    "        self.pcloud_zeros = 0 #count if there are zero points in an event\n",
    "        self.dimension = dimension \n",
    "        self.isotope = isotope\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self \n",
    "\n",
    "    def transform(self,X,y=None): #for up/down scaling\n",
    "        data,event_lengths = X #with shape (file,event_lenghts) X needs to be the only input to preserve the conventions of custom transformer\n",
    "        len_run = len(data)\n",
    "        # new_array_name = isotope + '_size' + str(sample_size) + '_sampled'\n",
    "        new_data = np.full((len_run, self.target_size, self.dimension), np.nan) \n",
    "\n",
    "        for i in tqdm.tqdm(range(len_run),desc=\"Up/Downscaling in progress\"): #\n",
    "            ev_len = event_lengths[i] #length of event-- i.e. number of instances\n",
    "            if ev_len == 0: #if event length is 0\n",
    "                print(f\"This event has 0 length: {i}\")\n",
    "                self.pcloud_zeros+=1\n",
    "                continue\n",
    "            if ev_len > self.target_size: #upsample\n",
    "                random_points = np.random.choice(ev_len, self.target_size, replace=False)  #choosing the random instances to sample\n",
    "                for r in range(len(random_points)):  # #only adds random sample_size points \n",
    "                    new_data[i,r] = data[i,random_points[r]]\n",
    "\n",
    "            else:\n",
    "                new_data[i,:ev_len,:] = data[i,:ev_len,:] #downsample\n",
    "                need = self.target_size - ev_len\n",
    "                random_points = np.random.choice(ev_len, need, replace= True if need > ev_len else False) #only repeats points more points needed than event length \n",
    "                count = ev_len\n",
    "                for r in random_points:\n",
    "                    new_data[i,count] = data[i,r]\n",
    "                    if np.isnan(new_data[i, count, 0]):\n",
    "                        print(f\"NaN found at event {i}, index {count}\") #need to make sure no nans remain\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        assert self.pcloud_zeros == 0, f\"There are {self.pcloud_zeros} events with no points\"\n",
    "        \n",
    "        assert new_data.shape == (len_run, self.target_size, self.dimension), 'Array has incorrect shape'\n",
    "        assert not np.isnan(new_data).any(), \"NaNs detected in new_data\" #very imporant to make sure there are no nans \n",
    "        \n",
    "        print(f\"Resampled shape of data: {new_data.shape}\")\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Scaling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalingData(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    None\n",
    "\n",
    "    Return\n",
    "    X (np.array): MinMaxScaler() applied data for all columns\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,dimension=4):\n",
    "        self.dimension = dimension\n",
    "        self.scalers = [MinMaxScaler(feature_range=(-1, 1)) for _ in range(dimension)]\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        for n in range(self.dimension):\n",
    "            data = X[:, :, n].reshape(-1, 1)\n",
    "            self.scalers[n].fit(data)\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        n_dict = {0:\"x\",1:\"y\",2:\"z\",3:\"charge\"}\n",
    "        for n in range(self.dimension):\n",
    "            data = X[:, :, n].reshape(-1, 1) #need to flatted the 2-D array first new shape (num_events*target_size)\n",
    "            X[:, :, n] = self.scalers[n].transform(data).reshape(X.shape[0], X.shape[1])\n",
    "            print(f\"Scaler min/max for {n_dict[n]}: {self.scalers[n].data_min_[0]}, {self.scalers[n].data_max_[0]}\")\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Forming the pipeline</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 800\n",
    "isotope = \"16O\"\n",
    "pipeline_1 = Pipeline([\n",
    "    (\"conversion\", ConvertingFiles(run)),\n",
    "    # (\"outlier\",OutlierDetection()), #getting rid of the outliers\n",
    "    (\"sampler\", UpDownScaling(target_size,isotope)),\n",
    "]) #up/down sampler \n",
    "\n",
    "# The `pipeline_2` is a data processing pipeline that consists of the following steps:\n",
    "pipeline_2 = Pipeline([\n",
    "    (\"scaling\", ScalingData()),\n",
    "]) #scaling (w/ concatonated dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline for Run 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stripping Point Clouds of ID information: 100%|██████████| 10608/10608 [00:02<00:00, 3579.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cloud_1927598' 'cloud_1927613' 'cloud_1927621' ... 'cloud_1992837'\n",
      " 'cloud_1992840' 'cloud_1992841']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline for Run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m data_static \u001b[38;5;241m=\u001b[39m pipeline_1\u001b[38;5;241m.\u001b[39mfit_transform(file_path)\n\u001b[0;32m---> 12\u001b[0m transformed_data \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_static\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe full transformed data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformed_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/research_space_spyral/.venv/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/research_space_spyral/.venv/lib/python3.11/site-packages/sklearn/pipeline.py:543\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    541\u001b[0m last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m    548\u001b[0m         Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    549\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/research_space_spyral/.venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/research_space_spyral/.venv/lib/python3.11/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mScalingData.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m,X,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdimension):\n\u001b[0;32m---> 16\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalers[n]\u001b[38;5;241m.\u001b[39mfit(data)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "eps = 1e-8\n",
    "\n",
    "for run in range(104,105):\n",
    "    if run < 100:\n",
    "        file_path = f\"/Volumes/researchEXT/O16/no_efield/PointcloudLegacy/run_00{run}.h5\"\n",
    "    else:\n",
    "        file_path = f\"/Volumes/researchEXT/O16/no_efield/PointcloudLegacy/run_0{run}.h5\"\n",
    "    file_exists = Path(file_path)\n",
    "    if file_exists.exists():\n",
    "        print(f\"Pipeline for Run {run}\")\n",
    "        data_static = pipeline_1.fit_transform(file_path)\n",
    "        transformed_data = pipeline_2.fit_transform(data_static)\n",
    "        print()\n",
    "        print(f\"The full transformed data shape: {transformed_data.shape}\")\n",
    "\n",
    "        mask = np.any((transformed_data > 1+eps) | (transformed_data < -1-eps), axis=(1, 2))\n",
    "        indices = np.argwhere(mask)\n",
    "        print(indices)\n",
    "        assert len(indices) == 0, \"Points remain that are not within range[-1,1]\"\n",
    "        \n",
    "        np.save(f\"/Volumes/researchEXT/O16/ml models/data_exp_pred/run{run}_{isotope}_size{target_size}_test_features.npy\", transformed_data) #FILE CHANGE\n",
    "        \n",
    "    else:\n",
    "        print(f\"Skipping run {run}, as it does not exist\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Saving to .npy files</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
